<!DOCTYPE html>
<html>

<head>
    <title>pytorch学习15-加入注意力机制的seq2seq优化流量预测</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="keywords" content="pytorch学习15-加入注意力机制的seq2seq优化流量预测, pytorch, AI, CodeShurrik" />
    <meta name="description" content="pytorch学习15-加入注意力机制的seq2seq优化流量预测, pytorch, AI, seq2seq序列到序列模型，是从一个序列生成另外一个序列。 它涉及两个过程：一个是理解前一个序列的编码器，另一个是用理解到的内容来生成新的序列的解码器。至于序列所采用的模型可以是RNN，LSTM，GRU，其它序列模型等。上述的过程和我们大脑理解东西的过程很相似，听到一句话，理解之后，尝试组装答案，进行回答，一般..." />
    <meta name="theme-color" content="#2CA6CB"/>
    <link rel="shortcut icon" type="image/x-icon" media="screen" href="http://localhost:4000/favicon.ico" />
    <link rel="canonical" href="http://localhost:4000/2022-12-03/pytorch%E5%AD%A6%E4%B9%A015-%E5%8A%A0%E5%85%A5%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84seq2seq%E4%BC%98%E5%8C%96%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B/" />
    <link rel="alternate" type="application/rss+xml" title="CodeShurrik" href="http://localhost:4000/feed.xml"  />

    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/bootstrap.css"/>
    <link rel="stylesheet" href="https://cdn.bootcss.com/octicons/3.5.0/octicons.min.css" >
    <!-- <script async defer data-website-id="bef53d4f-2117-4edd-b7a8-8754ea01f2c2" src="http://localhost:4000/static/js/umami.js"></script> -->
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/style.css" />
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/highlight.css" />
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/post.css" />
    

</head>


<body>

    <header>
        <nav class="navbar navbar-tiffany rectangle" role="navigation">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://localhost:4000/">CodeShurrik</a>
                    <p class="navbar-text">
                        <span id="typed"></span>
                        <script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.9"></script>
                        <script>
                            var typed = new Typed("#typed", {
                                strings: ['welcome to','shurrik blog'],
                                startDelay: 300,
                                typeSpeed: 100,
                                loop: true,
                                backSpeed: 50,
                                showCursor: true
                            });
                        </script>
                    </p>
                </div>
                <div class="collapse navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        
                        <li>
                        <a href="http://localhost:4000/" class="word-keep"><span class="octicon octicon-book"></span></span>&nbsp;&nbsp;Blog</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/category/" class="word-keep"><span class="octicon octicon-list-unordered"></span>&nbsp;&nbsp;Category</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/tags/" class="word-keep"><span class="octicon octicon-tag"></span>&nbsp;&nbsp;Tag</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/fm/" class="word-keep"><span class="octicon octicon-unmute"></span>&nbsp;&nbsp;FM</a>
                        </li>
                        
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/monitor/" class="word-keep"><span class="octicon octicon-tools"></span>&nbsp;&nbsp;Monitor</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/about/" class="word-keep"><span class="octicon octicon-terminal"></span>&nbsp;&nbsp;About</a>
                        </li>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                    </ul>
                </div>
            </div>
        </nav>
    </header>



<div class="main">
    <div class="container">
        <div class="row">
    <div class="content col-lg-9">
        <div class="sheet post">
          <header>
            <h2>pytorch学习15-加入注意力机制的seq2seq优化流量预测</h2>
            <p class="post-meta">
                <span class="octicon octicon-clock"></span> Dec 3, 2022
            </p>
            <p class="post-tag">
                <span><a href="http://localhost:4000/category/#pytorch"><span class="octicon octicon-list-unordered"></span>&nbsp;pytorch</a></span>
                <span>
                    <a class="word-keep" href="http://localhost:4000/tags/#AI"><span class="octicon octicon-tag"></span>&nbsp;AI</a>
                    
                </span>
            </p>
          </header>
          <hr class="boundary">
          <article>
            <p>seq2seq序列到序列模型，是从一个序列生成另外一个序列。 它涉及两个过程：一个是理解前一个序列的编码器，另一个是用理解到的内容来生成新的序列的解码器。至于序列所采用的模型可以是RNN，LSTM，GRU，其它序列模型等。上述的过程和我们大脑理解东西的过程很相似，<code class="language-plaintext highlighter-rouge">听到一句话，理解之后，尝试组装答案，进行回答</code>，一般用于机器翻译等NLP领域。当然用来优化我们的流量预测模型也是没问题的。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/209892572-2be0b459-5b51-4c8e-8e49-9f64953a246a.png" alt="seq2seq" /></p>

<h3 id="seq2seq的不足"><strong>seq2seq的不足</strong></h3>

<p>其他部分和直接用RNN建模区别不大，缺点还是比较明显的。由于context包含原始序列中的所有信息，它的长度就成了限制模型性能的瓶颈。除此之外，如果按照上述方式实现，只用到了编码器的最后一个隐层状态，信息利用率低下。另一方面RNN解码器根据context 生成输出目标序列， 然而，并非所有输入都对解码具有相同的贡献度，比如明天的预测流量可能受前1天的影响最大，受7天前影响最小 。 有什么方法能改变上下文变量context呢？ 答案就是注意力机制。</p>

<h3 id="注意力机制"><strong>注意力机制</strong></h3>

<p>Attention机制最早由Google机器学习团队于2014年提出，2017年Google机器翻译团队再次发表论文<a href="https://arxiv.org/abs/1706.03762">《Attention Is All You Need》</a>详细介绍了注意力机制，并在近年广泛的应用在深度学习中的各个领域，比如大名鼎鼎的Transformer、BERT。</p>

<p>简单理解，注意力即权重，权重就是注意力，注意力机制就是一个加权求和的计算过程。其中一种计算缩放点积注意力的公式如下：</p>

<p><img src="https://user-images.githubusercontent.com/4729226/205429606-713bc74e-047c-4f87-8da6-2a13226a2413.png" alt="attention" /></p>

<p>其中Q是query，K是key，V是value，dk为k的维度(由于Q和K长度相等，也等于Q的维度)。计算过程如下：</p>

<ol>
  <li>根据query和key计算两者的相似性或相关性，常见方法有求两者的向量点积、求两者cosine相似度、引入额外的神经网络来求值。</li>
  <li>对步骤1的原始分值进行softmax归一化处理后得到权重</li>
  <li>根据权重系数对value进行加权求和</li>
</ol>

<h3 id="注意力实现"><strong>注意力实现</strong></h3>

<p>缩放点积注意力的实现就是套入上述公式计算，使用了dropout进行模型正则化。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DotProductAttention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""缩放点积注意力机制"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># queries的形状：(batch_size，查询的个数，d)
</span>    <span class="c1"># keys的形状：(batch_size，键－值对的个数，d)
</span>    <span class="c1"># values的形状：(batch_size，键－值对的个数，值的维度)
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">keys</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">dk</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">attention_weights</span><span class="p">),</span> <span class="n">values</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="编码器"><strong>编码器</strong></h3>

<p>编码器将长度可变的输入序列转换成 形状固定的上下文变量c， 并且将输入序列的信息在该上下文变量中进行编码。实现中使用最后一个时间步的隐状态hidden即可，其shape为(num_layers, batch_size, hidden_size)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Seq2SeqEncoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>
</code></pre></div></div>

<h3 id="解码器"><strong>解码器</strong></h3>

<p>当实现解码器时， 我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。 这就要求使用RNN实现的编码器和解码器具有相同数量的层和隐藏单元。</p>

<p>编码器在所有时间步的最终层隐状态，将作为注意力的k和v。</p>

<p>在每个解码时间步骤中，解码器上一个时间步的最终层隐状态将用作q，实现中为了简单只预测一个时间步，所以使用编码器的最后时间步最终层隐状态。</p>

<p>为了进一步包含经过编码的输入序列的信息，注意力输出和解码器输入进行拼接（concat）作为解码器输入。在最后时刻输出的隐藏状态hidden的基础上，使用一个全连接层得到预测输出。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># add attention
</span><span class="k">class</span> <span class="nc">Seq2SeqDecoder</span><span class="p">(</span><span class="n">Decoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="o">+</span><span class="n">hidden_size</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">DotProductAttention</span><span class="p">(</span><span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">enc_outputs</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_state</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="c1"># enc_output.shape(b,s,h)    hidden_state.shape(num_layer,b,h)
</span>        <span class="n">enc_output</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden</span>
        <span class="c1"># 编码器最终时间步的全层隐状态，将作为初始化解码器的隐状态
</span>        <span class="c1"># 编码器在所有时间步的最终层隐状态，将作为注意力的k和v
</span>        <span class="c1"># 解码器上一个时间步的最终层隐状态将用作q
</span>        <span class="c1"># for i in x:
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># query.shape(b,1,h)
</span>        <span class="c1"># context.shape(b,1,h)
</span>        <span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>
        <span class="n">att_context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>      <span class="c1"># shape(b,s,h)
</span>        <span class="n">x_and_context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">att_context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># shape(b,s,i+h)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x_and_context</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention</span><span class="p">.</span><span class="n">attention_weights</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden_state</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_attention_weights</span>
</code></pre></div></div>

<h3 id="整体代码"><strong>整体代码</strong></h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">datetime</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>
<span class="kn">from</span> <span class="nn">django.db.models.query</span> <span class="kn">import</span> <span class="n">QuerySet</span>
<span class="kn">from</span> <span class="nn">.models</span> <span class="kn">import</span> <span class="n">ThpDataset</span>
<span class="kn">from</span> <span class="nn">.coder</span> <span class="kn">import</span> <span class="n">Encoder</span><span class="p">,</span> <span class="n">Decoder</span>

<span class="c1"># plt.style.use('seaborn-whitegrid')
# 显示中文
</span><span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'font.sans-serif'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'SimHei'</span>
<span class="n">plt</span><span class="p">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s">'axes.unicode_minus'</span><span class="p">]</span> <span class="o">=</span> <span class="bp">False</span>

<span class="c1"># Seq2Seq模型参数
</span><span class="n">DAY_SIZE</span> <span class="o">=</span> <span class="mi">142</span>
<span class="n">INPUT_SIZE</span> <span class="o">=</span> <span class="n">DAY_SIZE</span><span class="o">*</span><span class="mi">12</span>
<span class="n">HIDDEN_SIZE</span> <span class="o">=</span> <span class="mi">2048</span>
<span class="n">NUM_HEADS</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">MID_SIZE</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="n">OUTPUT_SIZE</span> <span class="o">=</span> <span class="n">DAY_SIZE</span><span class="o">*</span><span class="mi">1</span>
<span class="n">SEQ_LEN</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">NUM_LAYERS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">DROPOUT</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Seq2Seq训练参数
</span><span class="n">EPOCH</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">0.001</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda:0'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">DotProductAttention</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""缩放点积注意力机制"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># queries的形状：(batch_size，查询的个数，d)
</span>    <span class="c1"># keys的形状：(batch_size，键－值对的个数，d)
</span>    <span class="c1"># values的形状：(batch_size，键－值对的个数，值的维度)
</span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
        <span class="n">dk</span> <span class="o">=</span> <span class="n">keys</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">queries</span><span class="p">,</span> <span class="n">keys</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="n">dk</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">attention_weights</span><span class="p">),</span> <span class="n">values</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Seq2SeqEncoder</span><span class="p">(</span><span class="n">Encoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
                                <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span>

<span class="c1"># add multihead_attention
</span><span class="k">class</span> <span class="nc">Seq2SeqDecoder</span><span class="p">(</span><span class="n">Decoder</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">gru</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="o">+</span><span class="n">hidden_size</span><span class="p">,</span>
                                <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MultiheadAttention</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="n">NUM_HEADS</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">MID_SIZE</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">MID_SIZE</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_outputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">enc_outputs</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_state</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">hidden</span><span class="p">):</span>
        <span class="c1"># enc_output.shape(b,s,h)    hidden_state.shape(num_layer,b,h)
</span>        <span class="n">enc_output</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">hidden</span>
        <span class="c1"># 编码器最终时间步的全层隐状态，将作为初始化解码器的隐状态
</span>        <span class="c1"># 编码器在所有时间步的最终层隐状态，将作为注意力的k和v
</span>        <span class="c1"># 解码器上一个时间步的最终层隐状态将用作q
</span>        <span class="c1"># for i in x:
</span>        <span class="n">query</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">hidden_state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># query.shape(b,1,h)
</span>
        <span class="n">context</span><span class="p">,</span> <span class="n">attention_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">query</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">,</span> <span class="n">enc_output</span><span class="p">)</span>  <span class="c1"># shape(b,1,h)
</span>        <span class="n">att_context</span> <span class="o">=</span> <span class="n">context</span><span class="p">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>      <span class="c1"># 广播成shape(b,s,h)
</span>        <span class="n">x_and_context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">att_context</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># shape(b,s,i+h)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">gru</span><span class="p">(</span><span class="n">x_and_context</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear1</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear2</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">hidden_state</span>

    <span class="o">@</span><span class="nb">property</span>
    <span class="k">def</span> <span class="nf">attention_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">_attention_weights</span>


<span class="k">class</span> <span class="nc">EncoderDecoder</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="s">"""编码器-解码器架构的基类"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderDecoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enc_x</span><span class="p">,</span> <span class="n">dec_x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">enc_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc_x</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="n">dec_hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">enc_outputs</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec_x</span><span class="p">,</span> <span class="n">dec_hidden</span><span class="p">)</span>


<span class="c1"># 时间戳位置编码
</span><span class="k">def</span> <span class="nf">position_encoding</span><span class="p">(</span><span class="n">datetime</span><span class="p">):</span>
    <span class="n">position</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">day</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">day</span>
    <span class="n">hour</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">hour</span>
    <span class="n">minute</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">minute</span>

    <span class="n">days_in_month</span> <span class="o">=</span> <span class="mi">30</span>
    <span class="n">hours_in_day</span> <span class="o">=</span> <span class="mi">24</span>
    <span class="n">mins_in_hour</span> <span class="o">=</span> <span class="mi">60</span>

    <span class="n">position</span><span class="p">[</span><span class="s">'sin_day'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">day</span><span class="o">/</span><span class="n">days_in_month</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="s">'cos_day'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">day</span><span class="o">/</span><span class="n">days_in_month</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="s">'sin_hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">hour</span><span class="o">/</span><span class="n">hours_in_day</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="s">'cos_hour'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">hour</span><span class="o">/</span><span class="n">hours_in_day</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="s">'sin_minute'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">minute</span><span class="o">/</span><span class="n">mins_in_hour</span><span class="p">)</span>
    <span class="n">position</span><span class="p">[</span><span class="s">'cos_minute'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="p">.</span><span class="n">pi</span><span class="o">*</span><span class="n">minute</span><span class="o">/</span><span class="n">mins_in_hour</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">position</span>


<span class="k">def</span> <span class="nf">load_data</span><span class="p">(</span><span class="n">thpdataset_lines</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="c1"># 数据载入预处理
</span>    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">thpdataset_lines</span><span class="p">,</span> <span class="p">(</span><span class="n">QuerySet</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
                      <span class="p">),</span> <span class="s">'原始数据需传入QuerySet或list'</span>
    <span class="c1"># 组装原始数据，列表
</span>    <span class="n">src_data</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">thpdataset_line</span> <span class="ow">in</span> <span class="n">thpdataset_lines</span><span class="p">:</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">position_encoding</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">created_at</span><span class="p">)</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">device_num</span><span class="p">)</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">dxcc_usage</span><span class="p">)</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">thp_2</span><span class="p">)</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">thp_1</span><span class="p">)</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="s">'sin_day'</span><span class="p">])</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="s">'cos_day'</span><span class="p">])</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="s">'sin_hour'</span><span class="p">])</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="s">'cos_hour'</span><span class="p">])</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="s">'sin_minute'</span><span class="p">])</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">position</span><span class="p">[</span><span class="s">'cos_minute'</span><span class="p">])</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">mean_thp</span><span class="p">)</span>
        <span class="n">src_data</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">thpdataset_line</span><span class="p">.</span><span class="n">thp</span><span class="p">)</span>

    <span class="c1"># 列表转换为数组
</span>    <span class="n">np_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">src_data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">feature_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">'device_num'</span><span class="p">,</span> <span class="s">'dxcc_usage'</span><span class="p">,</span> <span class="s">'thp_2'</span><span class="p">,</span> <span class="s">'thp_1'</span><span class="p">,</span> <span class="s">'sin_day'</span><span class="p">,</span>
                     <span class="s">'cos_day'</span><span class="p">,</span> <span class="s">'sin_hour'</span><span class="p">,</span> <span class="s">'cos_hour'</span><span class="p">,</span> <span class="s">'sin_minute'</span><span class="p">,</span> <span class="s">'cos_minute'</span><span class="p">,</span> <span class="s">'mean_thp'</span><span class="p">,</span> <span class="s">'thp'</span><span class="p">]</span>

    <span class="c1"># 一维数组转换为矩阵, N行12列
</span>    <span class="n">np_data</span> <span class="o">=</span> <span class="n">np_data</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_names</span><span class="p">))</span>
    <span class="n">np_data</span> <span class="o">=</span> <span class="n">np_data</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">np_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">//</span><span class="n">DAY_SIZE</span><span class="p">)</span><span class="o">*</span><span class="n">DAY_SIZE</span><span class="p">:]</span>  <span class="c1"># 取整
</span>
    <span class="k">if</span> <span class="n">train</span><span class="p">:</span>
        <span class="c1"># 计算训练数据集的最大最小值
</span>        <span class="nb">max</span><span class="p">,</span> <span class="nb">min</span> <span class="o">=</span> <span class="n">np_data</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np_data</span><span class="p">.</span><span class="nb">min</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># 记录训练数据的归一化参数，在预测时对数据做反归一化
</span>        <span class="n">np</span><span class="p">.</span><span class="n">savez</span><span class="p">(</span><span class="s">'deeplearn/st_train_param.npz'</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="nb">max</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="nb">min</span><span class="p">)</span>
        <span class="c1"># 对数据进行归一化处理
</span>        <span class="n">np_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">np_data</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>
        <span class="c1"># 转换为 (n,142*12)
</span>        <span class="n">np_data</span> <span class="o">=</span> <span class="n">np_data</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">INPUT_SIZE</span><span class="p">)</span>
        <span class="c1"># 划分训练测试集
</span>        <span class="n">offset</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">np_data</span><span class="p">)</span>
        <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="n">np_data</span><span class="p">[:</span><span class="n">offset</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">np_data</span><span class="p">[</span><span class="n">offset</span><span class="o">-</span><span class="mi">8</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="nb">min</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 加载训练数据的最大最小值
</span>        <span class="n">seq2seq_train_param</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'deeplearn/st_train_param.npz'</span><span class="p">)</span>
        <span class="nb">max</span><span class="p">,</span> <span class="nb">min</span> <span class="o">=</span> <span class="n">seq2seq_train_param</span><span class="p">[</span><span class="s">'max'</span><span class="p">],</span> <span class="n">seq2seq_train_param</span><span class="p">[</span><span class="s">'min'</span><span class="p">]</span>
        <span class="c1"># 对数据进行归一化处理
</span>        <span class="n">np_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">np_data</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="nb">max</span> <span class="o">-</span> <span class="nb">min</span><span class="p">)</span>
        <span class="n">np_data</span> <span class="o">=</span> <span class="n">np_data</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">INPUT_SIZE</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">np_data</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="nb">min</span>


<span class="k">class</span> <span class="nc">Seq2SeqTrainDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">np_data</span><span class="p">):</span>
        <span class="c1"># 生成序列数据
</span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">extract_data</span><span class="p">(</span><span class="n">np_data</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">x_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># shape(n, seq_len, INPUT_SIZE)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">y_data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">from_numpy</span><span class="p">(</span>
            <span class="n">y</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">OUTPUT_SIZE</span><span class="p">)</span>  <span class="c1"># shape(n, OUTPUT_SIZE)
</span>
        <span class="c1"># 获得reshape后数据集行数n, 即batch_size
</span>        <span class="bp">self</span><span class="p">.</span><span class="nb">len</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">x_data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">extract_data</span><span class="p">(</span><span class="n">np_data</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="n">SEQ_LEN</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">np_data</span><span class="p">)</span> <span class="o">-</span> <span class="n">seq_len</span><span class="p">):</span>
            <span class="n">x</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np_data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">seq_len</span><span class="p">])</span>
            <span class="n">y</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">np_data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="n">seq_len</span><span class="p">])</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">OUTPUT_SIZE</span><span class="p">,</span> <span class="mi">12</span><span class="p">)[:,</span> <span class="p">:,</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>  <span class="c1"># 取流量那一列作为标签
</span>        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>

    <span class="c1"># 获得索引方法
</span>    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">x_data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="bp">self</span><span class="p">.</span><span class="n">y_data</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>

    <span class="c1"># 获得数据集长度
</span>    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="nb">len</span>


<span class="c1"># 模型
</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Seq2SeqEncoder</span><span class="p">(</span><span class="n">INPUT_SIZE</span><span class="p">,</span> <span class="n">HIDDEN_SIZE</span><span class="p">,</span> <span class="n">NUM_LAYERS</span><span class="p">,</span> <span class="n">DROPOUT</span><span class="p">)</span>
<span class="n">decoder</span> <span class="o">=</span> <span class="n">Seq2SeqDecoder</span><span class="p">(</span><span class="n">INPUT_SIZE</span><span class="p">,</span> <span class="n">HIDDEN_SIZE</span><span class="p">,</span>
                         <span class="n">OUTPUT_SIZE</span><span class="p">,</span> <span class="n">NUM_LAYERS</span><span class="p">,</span> <span class="n">DROPOUT</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">EncoderDecoder</span><span class="p">(</span><span class="n">encoder</span><span class="p">,</span> <span class="n">decoder</span><span class="p">)</span>
<span class="c1"># 损失函数
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="c1"># 优化器
</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>
<span class="c1"># 学习率调整器 adam不需手动调整学习率
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
#     optimizer=optimizer, verbose=True)
</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">():</span>
    <span class="c1"># 读取训练数据并划分训练集和测试集
</span>    <span class="n">now</span> <span class="o">=</span> <span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">()</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">now</span> <span class="o">-</span> <span class="n">datetime</span><span class="p">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">days</span><span class="o">=</span><span class="mi">60</span><span class="p">)</span>
    <span class="n">thpdataset_lines</span> <span class="o">=</span> <span class="n">ThpDataset</span><span class="p">.</span><span class="n">objects</span><span class="p">.</span><span class="nb">filter</span><span class="p">(</span>
        <span class="n">created_at__gte</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">created_at__lte</span><span class="o">=</span><span class="n">now</span><span class="p">,</span> <span class="n">perf_name</span><span class="o">=</span><span class="s">'水土'</span><span class="p">)</span>

    <span class="n">train_data</span><span class="p">,</span> <span class="n">test_data</span><span class="p">,</span> <span class="nb">max</span><span class="p">,</span> <span class="nb">min</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="n">thpdataset_lines</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Seq2SeqTrainDataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">)</span>
    <span class="n">test_dataset</span> <span class="o">=</span> <span class="n">Seq2SeqTrainDataset</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="n">dataset</span><span class="o">=</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">test_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">dataset</span><span class="o">=</span><span class="n">test_dataset</span><span class="p">,</span>
                             <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"[</span><span class="si">{</span><span class="n">now</span><span class="p">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d %H:%M:%S'</span><span class="p">)</span><span class="si">}</span><span class="s">]水土训练开始, 训练集长度为:</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">)</span><span class="si">}</span><span class="s">, 测试集长度为:</span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">exists</span><span class="p">(</span><span class="s">'deeplearn/st_model_param.pkl'</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'deeplearn/st_model_param.pkl'</span><span class="p">))</span>

    <span class="n">train_loss_list</span><span class="p">,</span> <span class="n">test_loss_list</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>
    <span class="n">best_loss</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">best_model_state</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="p">[:]</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">EPOCH</span><span class="p">):</span>
        <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">data</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">,</span> <span class="mi">0</span><span class="p">):</span>
            <span class="c1"># 准备数据dataloader会将按batch_size返回的数据整合成矩阵加载
</span>            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">data</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">labels</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="c1"># 强制学习
</span>            <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">inputs</span>
            <span class="n">y_pred</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
            <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
            <span class="c1"># if epoch % 10 == 0:
</span>            <span class="c1">#     print(epoch, i, loss.item())
</span>            <span class="c1"># 反向传播求梯度
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
            <span class="c1"># 更新
</span>            <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

        <span class="c1"># train_loss /= len(train_loader)
</span>        <span class="n">train_loss_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_loss</span><span class="p">)</span>
        <span class="c1"># 调整学习率
</span>        <span class="c1"># scheduler.step(train_loss)
</span>
        <span class="c1"># 测试
</span>        <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">dec_inputs</span> <span class="o">=</span> <span class="n">x</span>
            <span class="n">y_pred</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dec_inputs</span><span class="p">)</span>
            <span class="n">test_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>

        <span class="n">test_loss_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">test_loss</span><span class="p">)</span>

        <span class="c1"># y_base = x.reshape(-1, SEQ_LEN, DAY_SIZE, 12)[:, -1, :, -1]  #上一天的流量
</span>        <span class="c1"># 得到最好那次的模型
</span>        <span class="k">if</span> <span class="n">test_loss</span> <span class="o">&lt;</span> <span class="n">best_loss</span><span class="p">:</span>
            <span class="n">best_loss</span> <span class="o">=</span> <span class="n">test_loss</span>
            <span class="n">best_y_pred</span> <span class="o">=</span> <span class="n">y_pred</span>
            <span class="n">best_model_state</span> <span class="o">=</span> <span class="n">copy</span><span class="p">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">())</span>
            <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">decoder</span><span class="p">.</span><span class="n">attention_weights</span><span class="p">.</span><span class="n">cpu</span><span class="p">()</span>

        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">100</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s">'epoch:</span><span class="si">{</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s">/</span><span class="si">{</span><span class="n">EPOCH</span><span class="si">}</span><span class="s">, train_loss:</span><span class="si">{</span><span class="n">train_loss</span><span class="si">}</span><span class="s">, test_loss:</span><span class="si">{</span><span class="n">test_loss</span><span class="si">}</span><span class="s">, best_loss:</span><span class="si">{</span><span class="n">best_loss</span><span class="p">:.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>

    <span class="c1"># 保存模型参数
</span>    <span class="k">if</span> <span class="n">best_model_state</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">best_model_state</span><span class="p">,</span> <span class="s">'deeplearn/st_model_param.pkl'</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"[</span><span class="si">{</span><span class="n">datetime</span><span class="p">.</span><span class="n">datetime</span><span class="p">.</span><span class="n">now</span><span class="p">().</span><span class="n">strftime</span><span class="p">(</span><span class="s">'%Y-%m-%d %H:%M:%S'</span><span class="p">)</span><span class="si">}</span><span class="s">]水土模型保存完毕! attention_weights:</span><span class="si">{</span><span class="n">attention_weights</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
        <span class="c1"># 对测试结果做反归一化处理
</span>        <span class="n">best_y_pred</span> <span class="o">=</span> <span class="n">best_y_pred</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="nb">min</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="nb">min</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="nb">max</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="nb">min</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="o">+</span> <span class="nb">min</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="n">best_y_pred</span> <span class="o">=</span> <span class="n">best_y_pred</span><span class="o">*</span><span class="mi">8</span><span class="o">/</span><span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">*</span><span class="mi">8</span><span class="o">/</span><span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span><span class="o">/</span><span class="mi">1000</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'水土训练数据'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">'真实流量'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">best_y_pred</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">label</span><span class="o">=</span><span class="s">'预测流量'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'流量,Gb/s'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss_list</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'train_loss'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_loss_list</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'test_loss'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'损失值'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>

        <span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'deeplearn/st_test.png'</span><span class="p">)</span>
        <span class="n">plt</span><span class="p">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">print</span><span class="p">(</span><span class="s">'本次训练没有得到更新!'</span><span class="p">)</span>

</code></pre></div></div>

<p>最后打印attention权重，可以看出明天的预测流量受前1天数据影响的权重最大，7天前的最小，这也和实际情况相符。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/205431015-08ed9515-b5b8-45d5-bcac-a961a368ffd8.png" alt="image-20221203155736556" /></p>

          </article>
          <hr class="boundary">
          <div id="post-share" class="bdsharebuttonbox">
              <a href="#" class="bds_more" data-cmd="more"></a>
              <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
              <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
              <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
              <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
              <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
          </div>
        </div>
        <div class="pad-min"></div>
       <!-- <div id="post-comment" class="sheet post"> -->
        <!--PC和WAP自适应版-->
        <!-- <div id="SOHUCS" ></div>  -->
        <!-- <script type="text/javascript"> 
        (function(){ 
        var appid = 'cyt71nso6'; 
        var conf = 'prod_d224308500d6b59d843a38fa4a3b0fd9'; 
        var width = window.innerWidth || document.documentElement.clientWidth; 
        if (width < 960) { 
        window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>

        <div id="disqus_thread"></div> -->
        <!-- </div> -->
    </div>
    <div class="content-navigation col-lg-3">
      <div class="shadow-bottom-center" >
        <div class="content-navigation-toc">
            <div class="content-navigation-header">
                <span class="octicon octicon-list-unordered"></span>&nbsp;Toc
            </div>
            <div class="content-navigation-list toc"></div>
        </div>
        <div class="content-navigation-tag">
            <div class="content-navigation-header">
                <span class="octicon octicon-list-unordered"></span>&nbsp;Tags
            </div>
            <div class="content-navigation-list">
                <ul>
                    
                    <li>
                        <a href="http://localhost:4000/tags#AI"><span class="octicon octicon-tag"></span>&nbsp;AI</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="content-navigation-related">
            <div class="content-navigation-header">
                <span class="octicon octicon-list-unordered"></span>&nbsp;Related
            </div>
            <div class="content-navigation-list">
                <ul>
                    

                    

                    
                        
                            <li>
                                <a href="http://localhost:4000/2023-01-03/pytorch%E5%AD%A6%E4%B9%A016-transformer%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B/">pytorch学习16-transformer流量预测</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-11-09/pytorch%E5%AD%A6%E4%B9%A014-%E4%BD%BF%E7%94%A8GPU%E8%AE%AD%E7%BB%83/">pytorch学习14-使用GPU训练</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-10-16/pytorch%E5%AD%A6%E4%B9%A013-%E4%BD%BF%E7%94%A8RNN%E4%BC%98%E5%8C%96%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B/">pytorch学习13-使用RNN优化流量预测</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-10-14/pytorch%E5%AD%A6%E4%B9%A012-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/">pytorch学习12-循环神经网络基础</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-10-09/pytorch%E5%AD%A6%E4%B9%A011-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/">pytorch学习11-卷积神经网络基础</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-30/pytorch%E5%AD%A6%E4%B9%A010-%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B%E5%AE%9E%E6%88%98/">pytorch学习10-流量预测实战</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-29/pytorch%E5%AD%A6%E4%B9%A09-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/">pytorch学习9-多分类问题</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-27/pytorch%E5%AD%A6%E4%B9%A08-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/">pytorch学习8-加载数据集</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-26/pytorch%E5%AD%A6%E4%B9%A07-%E5%A4%84%E7%90%86%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5/">pytorch学习7-处理多维特征的输入</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-23/pytorch%E5%AD%A6%E4%B9%A06-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98Logistic%E5%9B%9E%E5%BD%92/">pytorch学习6-分类问题Logistic回归</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-22/pytorch%E5%AD%A6%E4%B9%A05-pytorch%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">pytorch学习5-pytorch实现线性回归</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-21/pytorch%E5%AD%A6%E4%B9%A04-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">pytorch学习4-反向传播</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-20/pytorch%E5%AD%A6%E4%B9%A03-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/">pytorch学习3-梯度下降算法</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-15/pytorch%E5%AD%A6%E4%B9%A02-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">pytorch学习2-线性模型</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-13/pytorch%E5%AD%A6%E4%B9%A01-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/">pytorch学习1-人工智能基础</a>
                            </li>
                        
                    
                </ul>
            </div>
        </div>
      </div>
    </div>
</div>
    </div>
    
    <div class="page-scrollTop" data-toggle="tooltip" data-placement="top" title="Top">
        <a href="javascript:void(0);">
            <div class="arrow"></div>
            <div class="stick"></div>
        </a>
    </div>
</div>

    <footer  class="footnote footnote-tiffany">
        <div class="container">
                <a class="foot-item" href="mailto:shurrik.dly@gmail.com" target="_blank"><span class="octicon octicon-mail"></span></a>
                <a class="foot-item" href="https://github.com/ixjx" target="_blank"><span class="octicon octicon-mark-github"></span></a>
                <a class="foot-item" href="http://localhost:4000/feed.xml" target="_blank"><span class="octicon octicon-rss"></span></a>
                <a class="foot-item" href="http://localhost:4000/link/"><span class="octicon octicon-link-external"></span></a>
                &nbsp;
                <a href="https://ixjx.github.io/"><span class="word-keep">&copy; Codeshurrik</span></a>
                <br>
                <span>珍爱生命，远离IE。请使用Chrome或Safari，你懂的 :)</span>
                <br>
                <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
                <script>// <![CDATA[
                    var now = new Date(); function createtime(){ var grt= new Date("07/28/2013 21:00:00");now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} document.getElementById("timeDate").innerHTML = "已安全运行"+dnum+"天"; document.getElementById("times").innerHTML = hnum + "小时" + mnum + "分" + snum + "秒"; } setInterval("createtime()",250);
                // ]]></script>
        </div>
    </footer>
    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
    <script type="text/javascript" src="https://cdn.bootcss.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
    <!-- <script id="MathJax-script" async src="https://cdn.bootcss.com/mathjax/3.0.5/es5/tex-mml-chtml.js"></script> -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    
    <script type="text/javascript" src="http://localhost:4000/static/js/script.js"></script>
    <script type="text/javascript" src="http://localhost:4000/static/js/post.js"></script>
    


</body>
</html>
