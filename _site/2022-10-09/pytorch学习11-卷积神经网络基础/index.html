<!DOCTYPE html>
<html>

<head>
    <title>pytorch学习11-卷积神经网络基础</title>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="keywords" content="pytorch学习11-卷积神经网络基础, pytorch, AI, CodeShurrik" />
    <meta name="description" content="pytorch学习11-卷积神经网络基础, pytorch, AI, 卷积神经网络 CNN" />
    <meta name="theme-color" content="#2CA6CB"/>
    <link rel="shortcut icon" type="image/x-icon" media="screen" href="http://localhost:4000/favicon.ico" />
    <link rel="canonical" href="http://localhost:4000/2022-10-09/pytorch%E5%AD%A6%E4%B9%A011-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/" />
    <link rel="alternate" type="application/rss+xml" title="CodeShurrik" href="http://localhost:4000/feed.xml"  />

    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/bootstrap.css"/>
    <link rel="stylesheet" href="https://cdn.bootcss.com/octicons/3.5.0/octicons.min.css" >
    <!-- <script async defer data-website-id="bef53d4f-2117-4edd-b7a8-8754ea01f2c2" src="http://localhost:4000/static/js/umami.js"></script> -->
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/style.css" />
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/highlight.css" />
    <link rel="stylesheet" type="text/css" href="http://localhost:4000/static/css/post.css" />
    

</head>


<body>

    <header>
        <nav class="navbar navbar-tiffany rectangle" role="navigation">
            <div class="container">
                <div class="navbar-header">
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="http://localhost:4000/">CodeShurrik</a>
                    <p class="navbar-text">
                        <span id="typed"></span>
                        <script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.9"></script>
                        <script>
                            var typed = new Typed("#typed", {
                                strings: ['welcome to','shurrik blog'],
                                startDelay: 300,
                                typeSpeed: 100,
                                loop: true,
                                backSpeed: 50,
                                showCursor: true
                            });
                        </script>
                    </p>
                </div>
                <div class="collapse navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        
                        <li>
                        <a href="http://localhost:4000/" class="word-keep"><span class="octicon octicon-book"></span></span>&nbsp;&nbsp;Blog</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/category/" class="word-keep"><span class="octicon octicon-list-unordered"></span>&nbsp;&nbsp;Category</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/tags/" class="word-keep"><span class="octicon octicon-tag"></span>&nbsp;&nbsp;Tag</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/fm/" class="word-keep"><span class="octicon octicon-unmute"></span>&nbsp;&nbsp;FM</a>
                        </li>
                        
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/monitor/" class="word-keep"><span class="octicon octicon-tools"></span>&nbsp;&nbsp;Monitor</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="http://localhost:4000/about/" class="word-keep"><span class="octicon octicon-terminal"></span>&nbsp;&nbsp;About</a>
                        </li>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                    </ul>
                </div>
            </div>
        </nav>
    </header>



<div class="main">
    <div class="container">
        <div class="row">
    <div class="content col-lg-9">
        <div class="sheet post">
          <header>
            <h2>pytorch学习11-卷积神经网络基础</h2>
            <p class="post-meta">
                <span class="octicon octicon-clock"></span> Oct 9, 2022
            </p>
            <p class="post-tag">
                <span><a href="http://localhost:4000/category/#pytorch"><span class="octicon octicon-list-unordered"></span>&nbsp;pytorch</a></span>
                <span>
                    <a class="word-keep" href="http://localhost:4000/tags/#AI"><span class="octicon octicon-tag"></span>&nbsp;AI</a>
                    
                </span>
            </p>
          </header>
          <hr class="boundary">
          <article>
            <h2 id="卷积神经网络-cnn"><strong>卷积神经网络 CNN</strong></h2>

<h3 id="全连接"><strong>全连接</strong></h3>

<p>前篇中的<strong>完全由线性层串行而形成的网络层</strong>为全连接神经网络，即对于某一层的每个输出都将作为下一层的输入。对下一层而言，<strong>每一个输入值和每一个输出值之前都存在权重</strong>。</p>

<p>在全连接层中，实际上是把原先空间状态上的信息，转换为了一维的信息，使得原有的空间相对位置所蕴含的信息丢失。</p>

<p>下文仍以MNIST数据集为例。</p>

<h3 id="怎么卷积"><strong>怎么卷(积)</strong></h3>

<p>卷积实际上是把原始图像仍然按照空间的结构来进行保存数据。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194742506-7144bcf5-5472-4233-b7ce-a951e5766b14.png" alt="image" /></p>

<h4 id="卷积convolution过程"><strong>卷积(Convolution)过程</strong></h4>

<p><img src="https://user-images.githubusercontent.com/4729226/194742567-e8e60cdc-1e3d-4b93-b161-66b134667b90.png" alt="image" /></p>

<p>1*28*28指C(channel)* W(width) * H(height)。即通道数 * 图像宽度 * 图像高度，通道可以理解为层数，通过同样大小的多层图像堆叠才形成了最原始的图。</p>

<p>可以抽象的理解成原先的图是一个立方体性质的，卷积是将立方体的长宽高按照新的比例进行重新分割而成的。</p>

<p>如下图所示，底层是一个3 * W * H的原始图像，卷积的处理是每次对其中一个Patch进行处理，也就是从原数图像的左上角开始依次抽取一个3 * W’ * H’的图像对其进行卷积，输出一个C’ * W’’ * H’‘的子图。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194742776-2f5b82ae-61a6-46b6-a0e4-ad5902c211a7.png" alt="image" /></p>

<h4 id="下采样subsampling"><strong>下采样（Subsampling）</strong></h4>

<p>下采样的目的是减少特征图像的数据量，降低运算需求。在下采样过程中，通道数（Channel）保持不变，图像的宽度和高度发生改变。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194742824-e149a5b9-9bac-4a29-95a1-68ea79b74ee3.png" alt="image" /></p>

<h4 id="全连接层fully-connected"><strong>全连接层（Fully Connected）</strong></h4>

<p>先将原先多维的卷积结果通过全连接层转为一维的向量，再通过多层全连接层将原向量转变为可供输出的向量。</p>

<p>在前文的卷积过程与下采样过程，实际上是一种特征提取的手段或者过程，真正用于分类的过程是后续的全连接层。</p>

<h3 id="卷积原理"><strong>卷积原理</strong></h3>

<h4 id="单通道卷积"><strong>单通道卷积</strong></h4>

<p>设定对于规格为1 * W * H的原图，利用一个规格为1 * W’ * H’的卷积核进行卷积处理的数乘操作。</p>

<p>则需要从原始数据的左上角开始依次选取<strong>与核的规格相同</strong>(1 * W’ * H’)的输入数据进行数乘操作，并将求得的数值作为一个Output值进行填充。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194742944-fdce52cd-c9c1-4dc9-90d5-8736a60b370a.png" alt="image" /></p>

<p>Patch在原图上进行滑动时，每次只滑动一个像素，即包含重复计算的部分</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194742964-d9598f96-5d1b-439c-8083-2b786627c83e.png" alt="image" /></p>

<p>最后求得的Output的像素矩阵，即是对原图像，在设定的卷积核下的卷积结果，是一个规格为1 * W’ * H’的图像。</p>

<h4 id="多通道卷积"><strong>多通道卷积</strong></h4>

<p>对于多通道图像(N * W * H)，每一个通道是一个单通道的图像（1 * W * H）都要有一个自己的卷积核（1 * W’ * H’）来进行卷积。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194743057-58343fa2-addf-4656-a267-6bb874ec0cb1.png" alt="image" /></p>

<p>对于分别求出来的矩阵，需要再次进行求和才能得到最后的输出矩阵，最终的输出矩阵仍然是一个1 * W’ * H’的图像。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194743093-a697f858-6235-4843-923a-37ec1e284645.png" alt="image" /></p>

<p>将平面的图像转为立体的角度即如下图</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194743134-4d33ab17-a30b-4a44-a44f-34925a17e01a.png" alt="image" /></p>

<h4 id="改进多通道"><strong>改进多通道</strong></h4>

<p>多通道卷积中，每次只能把N个通道转变为1个通道，而无法在通道这个维度进行增加或降低。</p>

<p>因此，为了对通道进行更加灵活的操作，可以将原先N * W * H的图像，利用不同的卷积核对其多次求卷积，由于每次求卷积之后的输出图像为1 * W’ * H’，若一共求解了M次，即可以将此M次的求解结果<strong>按顺序在通道（Channel）这一维度上</strong>进行拼接，以此来形成一个规格为M * W’ * H’的图像。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194743289-a7b3d97c-8981-4be2-b2c7-cd16a266bc20.png" alt="image" /></p>

<h4 id="总结"><strong>总结</strong></h4>

<ol>
  <li>每个卷积核的通道数与输入通道数一致</li>
  <li>卷积核的数量与输出通道数一致</li>
  <li>卷积核的大小与图像大小无关</li>
</ol>

<p>上述中所提到的卷积核，是指的多通道的卷积核，而非前文中提到的二维的。</p>

<p><strong>综上所述</strong>为了使下图所表征的过程成立，即若需要使得原本为
\(n \times width_{in} \times height_{in}\)
的图像转变为一个
\(m \times width_{out} \times height_{out}\)
的图像，可以利用m个大小为
\(n \times kernel\_size_{width} \times kernel\_size_{height}\)
的卷积核。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194743406-6aaeed1b-b593-42e6-a183-1977eba6a142.png" alt="image" /></p>

<p>在实际操作中，即可抽象为利用一个四维张量作为卷积核，此四维张量的大小为
\(m \times n \times kernel\_size_{width} \times kernel\_size_{height}\)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span>
<span class="n">width</span><span class="p">,</span> <span class="n">height</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span>

<span class="n">kernel_size</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1">#默认转为3*3，最好用奇数正方形
</span>
<span class="c1">#在pytorch中的数据处理都是通过batch来实现的
#因此对于C*W*H的三个维度图像，在代码中实际上是一个B（batch）*C*W*H的四个维度的图像
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1">#生成一个四维的随机数
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">)</span>

<span class="c1">#Conv2d需要设定，输入输出的通道数以及卷积核尺寸
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">conv_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="卷积改进"><strong>卷积改进</strong></h3>

<h4 id="padding"><strong>Padding</strong></h4>

<p>若对于一个大小为N * N的原图，经过大小为M * M的卷积核卷积后，仍然想要得到一个大小为N * N的图像，则需要对原图进行Padding，即外围填充。</p>

<p>例如，对于一个5 * 5的原图，若想使用一个3 * 3的卷积核进行卷积，并获得一个同样5 * 5的图像，则需要进行Padding，<strong>通常外围填充0</strong></p>

<p><img src="https://user-images.githubusercontent.com/4729226/194743804-a15fb4de-8406-4df6-a645-7a576ab02e2c.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span>
         <span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span>
         <span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span>
         <span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span>
         <span class="mi">3</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">#将输入变为B*C*W*H
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1">#偏置量bias置为false
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">#将卷积核变为CI*CO*W*H
</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1">#将做出来的卷积核张量，赋值给卷积运算中的权重（参与卷积计算）
</span><span class="n">conv_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">.</span><span class="n">data</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/4729226/194743866-23d7ad78-1bb8-4adb-b89f-5590c5d75142.png" alt="微信截图_20221009153129" /></p>

<h4 id="stride"><strong>Stride</strong></h4>

<p>本质上即是Batch的步长，在Batch进行移动时，每次移动Stride的距离，以此来有效降低图像的宽度与高度。</p>

<p>例如，对于一个5 * 5的原图，若想使用一个3 * 3的卷积核进行卷积，并获得一个2 * 2的图像，则需要进行Stride，且Stride=2</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span>
         <span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span>
         <span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span>
         <span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span>
         <span class="mi">3</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>

<span class="c1">#将输入变为B*C*W*H
</span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1">#偏置量bias置为false
</span><span class="n">conv_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1">#将卷积核变为CI*CO*W*H
</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1">#将做出来的卷积核张量，赋值给卷积运算中的权重（参与卷积计算）
</span><span class="n">conv_layer</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">.</span><span class="n">data</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">conv_layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="下采样过程"><strong>下采样过程</strong></h3>

<h4 id="最大池化层maxpooling"><strong>最大池化层（MaxPooling）</strong></h4>

<p>对于一个M * M图像而言，通过最大池化层可以有效降低其宽度和高度上的数据量，例如通过一个N * N的最大池化层，即将原图分为若干个N * N大小的子图，并在其中选取最大值填充到输出图中，此时输出图的大小为M/N * M/N。</p>

<p><img src="https://user-images.githubusercontent.com/4729226/194744018-e3f48d12-a448-44be-b361-ed75a29c0c8c.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="nb">input</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span>
         <span class="mi">2</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span>
         <span class="mi">1</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span>
         <span class="mi">9</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">6</span><span class="p">]</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="nb">input</span><span class="p">).</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1">#kernel_size=2 则MaxPooling中的Stride也为2
</span><span class="n">maxpooling_layer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">maxpooling_layer</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="简单卷积神经网络的实现"><strong>简单卷积神经网络的实现</strong></h3>

<h4 id="模型图"><strong>模型图</strong></h4>

<p><img src="https://user-images.githubusercontent.com/4729226/194744074-7134d970-8ffc-4a02-9866-2e0ad46674ff.png" alt="image" /></p>

<h4 id="代码"><strong>代码</strong></h4>

<p><img src="https://user-images.githubusercontent.com/4729226/194744091-9570ce26-ac2f-45b6-9864-091d79a6c6fc.png" alt="image" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">MaxPool2d</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># Flatten data from (n,1,28,28) to (n,784)
</span>        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">pooling</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>   <span class="c1"># flatten
</span>        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div>

<p><img src="https://user-images.githubusercontent.com/4729226/194744722-976690d8-1317-4ccd-a662-1f325e0b08c9.png" alt="微信截图_20221009155254" /></p>

<p>本文参考自<a href="https://www.bilibili.com/video/BV1Y7411d7Ys?p=10">B站《PyTorch深度学习实践》P10</a></p>

          </article>
          <hr class="boundary">
          <div id="post-share" class="bdsharebuttonbox">
              <a href="#" class="bds_more" data-cmd="more"></a>
              <a href="#" class="bds_tsina" data-cmd="tsina" title="分享到新浪微博"></a>
              <a href="#" class="bds_weixin" data-cmd="weixin" title="分享到微信"></a>
              <a href="#" class="bds_douban" data-cmd="douban" title="分享到豆瓣网"></a>
              <a href="#" class="bds_fbook" data-cmd="fbook" title="分享到Facebook"></a>
              <a href="#" class="bds_copy" data-cmd="copy" title="分享到复制网址"></a>
          </div>
        </div>
        <div class="pad-min"></div>
       <!-- <div id="post-comment" class="sheet post"> -->
        <!--PC和WAP自适应版-->
        <!-- <div id="SOHUCS" ></div>  -->
        <!-- <script type="text/javascript"> 
        (function(){ 
        var appid = 'cyt71nso6'; 
        var conf = 'prod_d224308500d6b59d843a38fa4a3b0fd9'; 
        var width = window.innerWidth || document.documentElement.clientWidth; 
        if (width < 960) { 
        window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>

        <div id="disqus_thread"></div> -->
        <!-- </div> -->
    </div>
    <div class="content-navigation col-lg-3">
      <div class="shadow-bottom-center" >
        <div class="content-navigation-toc">
            <div class="content-navigation-header">
                <span class="octicon octicon-list-unordered"></span>&nbsp;Toc
            </div>
            <div class="content-navigation-list toc"></div>
        </div>
        <div class="content-navigation-tag">
            <div class="content-navigation-header">
                <span class="octicon octicon-list-unordered"></span>&nbsp;Tags
            </div>
            <div class="content-navigation-list">
                <ul>
                    
                    <li>
                        <a href="http://localhost:4000/tags#AI"><span class="octicon octicon-tag"></span>&nbsp;AI</a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="content-navigation-related">
            <div class="content-navigation-header">
                <span class="octicon octicon-list-unordered"></span>&nbsp;Related
            </div>
            <div class="content-navigation-list">
                <ul>
                    

                    

                    
                        
                            <li>
                                <a href="http://localhost:4000/2023-01-03/pytorch%E5%AD%A6%E4%B9%A016-transformer%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B/">pytorch学习16-transformer流量预测</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-12-03/pytorch%E5%AD%A6%E4%B9%A015-%E5%8A%A0%E5%85%A5%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84seq2seq%E4%BC%98%E5%8C%96%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B/">pytorch学习15-加入注意力机制的seq2seq优化流量预测</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-11-09/pytorch%E5%AD%A6%E4%B9%A014-%E4%BD%BF%E7%94%A8GPU%E8%AE%AD%E7%BB%83/">pytorch学习14-使用GPU训练</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-10-16/pytorch%E5%AD%A6%E4%B9%A013-%E4%BD%BF%E7%94%A8RNN%E4%BC%98%E5%8C%96%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B/">pytorch学习13-使用RNN优化流量预测</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-10-14/pytorch%E5%AD%A6%E4%B9%A012-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/">pytorch学习12-循环神经网络基础</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-30/pytorch%E5%AD%A6%E4%B9%A010-%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B%E5%AE%9E%E6%88%98/">pytorch学习10-流量预测实战</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-29/pytorch%E5%AD%A6%E4%B9%A09-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/">pytorch学习9-多分类问题</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-27/pytorch%E5%AD%A6%E4%B9%A08-%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/">pytorch学习8-加载数据集</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-26/pytorch%E5%AD%A6%E4%B9%A07-%E5%A4%84%E7%90%86%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5/">pytorch学习7-处理多维特征的输入</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-23/pytorch%E5%AD%A6%E4%B9%A06-%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98Logistic%E5%9B%9E%E5%BD%92/">pytorch学习6-分类问题Logistic回归</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-22/pytorch%E5%AD%A6%E4%B9%A05-pytorch%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">pytorch学习5-pytorch实现线性回归</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-21/pytorch%E5%AD%A6%E4%B9%A04-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/">pytorch学习4-反向传播</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-20/pytorch%E5%AD%A6%E4%B9%A03-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/">pytorch学习3-梯度下降算法</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-15/pytorch%E5%AD%A6%E4%B9%A02-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/">pytorch学习2-线性模型</a>
                            </li>
                        
                            <li>
                                <a href="http://localhost:4000/2022-09-13/pytorch%E5%AD%A6%E4%B9%A01-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/">pytorch学习1-人工智能基础</a>
                            </li>
                        
                    
                </ul>
            </div>
        </div>
      </div>
    </div>
</div>
    </div>
    
    <div class="page-scrollTop" data-toggle="tooltip" data-placement="top" title="Top">
        <a href="javascript:void(0);">
            <div class="arrow"></div>
            <div class="stick"></div>
        </a>
    </div>
</div>

    <footer  class="footnote footnote-tiffany">
        <div class="container">
                <a class="foot-item" href="mailto:shurrik.dly@gmail.com" target="_blank"><span class="octicon octicon-mail"></span></a>
                <a class="foot-item" href="https://github.com/ixjx" target="_blank"><span class="octicon octicon-mark-github"></span></a>
                <a class="foot-item" href="http://localhost:4000/feed.xml" target="_blank"><span class="octicon octicon-rss"></span></a>
                <a class="foot-item" href="http://localhost:4000/link/"><span class="octicon octicon-link-external"></span></a>
                &nbsp;
                <a href="https://ixjx.github.io/"><span class="word-keep">&copy; Codeshurrik</span></a>
                <br>
                <span>珍爱生命，远离IE。请使用Chrome或Safari，你懂的 :)</span>
                <br>
                <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
                <script>// <![CDATA[
                    var now = new Date(); function createtime(){ var grt= new Date("07/28/2013 21:00:00");now.setTime(now.getTime()+250); days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} document.getElementById("timeDate").innerHTML = "已安全运行"+dnum+"天"; document.getElementById("times").innerHTML = hnum + "小时" + mnum + "分" + snum + "秒"; } setInterval("createtime()",250);
                // ]]></script>
        </div>
    </footer>
    <script type="text/javascript" src="https://cdn.bootcss.com/jquery/1.11.3/jquery.min.js"></script>
    <script type="text/javascript" src="https://cdn.bootcss.com/bootstrap/3.3.0/js/bootstrap.min.js"></script>
    <!-- <script id="MathJax-script" async src="https://cdn.bootcss.com/mathjax/3.0.5/es5/tex-mml-chtml.js"></script> -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.css" integrity="sha384-bYdxxUwYipFNohQlHt0bjN/LCpueqWz13HufFEV1SUatKs1cm4L6fFgCi1jT643X" crossorigin="anonymous">
    <!-- The loading of KaTeX is deferred to speed up page rendering -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/katex.min.js" integrity="sha384-Qsn9KnoKISj6dI8g7p1HBlNpVx0I8p1SvlwOldgi3IorMle61nQy4zEahWYtljaz" crossorigin="anonymous"></script>
    <!-- To automatically render math in text elements, include the auto-render extension: -->
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
    
    <script type="text/javascript" src="http://localhost:4000/static/js/script.js"></script>
    <script type="text/javascript" src="http://localhost:4000/static/js/post.js"></script>
    


</body>
</html>
