---
layout: post
title: pytorch学习14-使用GPU训练
categories: pytorch
tags: AI
---





之前一直是使用CPU进行深度学习训练，最近申请下来了一块Tesla T4 GPU卡，不过是vGPU。在经历了一系列grid特殊驱动、license、重装gpu版pytorch之类的踩坑之后，终于实现了使用GPU进行训练，~~开启疯狂调参之路~~。速度从原来CPU训练一轮20分钟变成了7分钟左右，CUDA加速提升还是十分可观。

![gpu](https://user-images.githubusercontent.com/4729226/209892878-037111e0-6798-4467-be14-79688b4c065e.png)

代码改动，pytorch使用GPU的接口很简单：

```python
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
# 后面使用
model.to(device)
x, y = x.to(device), y.to(device)
```

model和tensor的to()有点不一样，model的to()没有返回值，是in place直接生效的，这一点需要注意下。

还有一个问题是预测的时候是通过HTTP接口，如果使用GPU预测，多来几次请求显存直接GG。暂时也没想到好办法，只有采用GPU训练+CPU预测的方式，训练完成保存模型之前调用一次`model.to(torch.device('cpu'))`，把模型参数放回CPU。
