---
layout: post
title: pytorch学习3-梯度下降算法
categories: pytorch
tags: AI

---



## **梯度下降（Gradient Descent）**

### **问题背景**

#### **穷举法**

在《pytorch学习2-线性模型》中，所使用的思想基于穷举，即提前已经设定好参数的准确值在某个区间内并以某个步长进行穷举（`np.arange(0.0, 4.1, 0.1)`）。

这样的思想在多维的情况下，即多个参数的时候，会引起维度诅咒的情况，在一个N维曲面中找一个最低点。使得原问题变得不可解。基于这样的问题，需要进行改进。

#### **分治法**

大化小，小化无，先对整体进行分割采样，在相对最低点进行进一步采样，直到其步长与误差符合条件。

但分治法有两个缺点

1. 容易只找到局部最优解，而不易找到一个全局最优解
2. 如果需要分得更加细致，则计算量仍然巨大



### **梯度下降算法**

梯度即导数变化最大的值，其方向为导数变化最大的方向。

$$
\frac{\partial f}{\partial x}=\lim_{\triangle x\rightarrow\infty}\frac{f(x+\triangle x)-f(x)}{\triangle x}
$$


若设△x>0，则对于增函数，梯度为上升方向，对于减函数，梯度为下降方向。如此方向都不是离极值点渐进的方向，因此需要取梯度下降的方向即梯度的反方向作为变化方向。



以凸函数为例，对于当前所选择的 ω 的值，显然并不是最低点。而对于将来要到达的全局的最低点，当前点只能向下取值，即向梯度下降方向变化。

![image-20210301162458062](https://user-images.githubusercontent.com/4729226/191153925-a42d8071-e634-480b-9013-14839d0c788b.png)

则取值点需要向下更新，所取的梯度即为
$$
\frac{\partial cost}{\partial \omega}
$$
，更新的公式为




$$
\omega = \omega - \alpha \frac{\partial cost}{\partial \omega}
$$




其中 α 为学习率即所下降的步长，不宜取太大。

![image-20210301163808129](https://user-images.githubusercontent.com/4729226/191155781-6fa81900-3359-48e3-af47-0919815896cf.png)



#### **局限性**

1. 梯度下降算法容易进入局部最优解（非凸函数），但是实际问题中的局部最优点较少，或已经基本可以当成全局最优点
2. 梯度下降算法容易陷入鞍点（当曲线水平时，梯度无法下降）



## **随机梯度下降(Stochastic Gradient Descent, SGD)**

随机选单个样本的损失为标准

即原公式变为


$$
\omega = \omega - \alpha \frac{\partial loss}{\partial \omega}
$$


其中


$$
\frac{\partial loss_n}{\partial \omega} = 2 x_n(x_n \omega - y_n)
$$


### **优势**

有可能跨越鞍点（神经网络常用）

随机梯度下降法在神经网络中被证明是有效的。效率较低(时间复杂度较高)，学习性能较好。



## **批量梯度下降（mini-batch）**

在前面的阐述中，普通的梯度下降算法利用数据整体，不容易避免鞍点，算法性能上欠佳，但算法效率高。

随机梯度下降需要利用每个的单个数据，虽然算法性能上良好，但计算过程环环相扣无法将样本抽离开并行运算，因此算法效率低，时间复杂度高。

综上可采取一种折中的方法，即批量梯度下降方法。

将若干个样本分为一组，记录一组的梯度用以代替随机梯度下降中的单个样本。

**该方法最为常用，也是默认接口**



本文参考自[B站《PyTorch深度学习实践》P3](https://www.bilibili.com/video/BV1Y7411d7Ys?p=3)

