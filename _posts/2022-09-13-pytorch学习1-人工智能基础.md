---
layout: post
title: pytorch学习1-人工智能基础
categories: pytorch
tags: AI

---



## **前置条件**

1. 线性代数 + 概率论
2. python基础

## **人工智能基础**

所要实现的智能具体上可分为**推理**与**预测**两大部分，总的来说即是利用计算机来代替人脑进行此类工作。

### **推理类**

根据**已经有的信息**来进行**推理**并作出**决策**的过程。

比如通过外界信息的情况（食堂做得菜，自己剩的钱...）来判断自己能够吃什么。

![机器学习-中午吃什么](https://user-images.githubusercontent.com/4729226/189840952-31d81d62-adea-4628-89ca-35e3fe872f65.jpg)



### **预测类**

根据**具体**的实体将其与**抽象**概念结合起来。

比如通过认识到猫这一动物实体，来与汉字“猫”建立联系，本质上是一个预测过程，赋予真实存在的事物一个并不真实存在的符号。

![机器学习-识别猫](https://user-images.githubusercontent.com/4729226/189840976-e7c2d4ba-5fec-4eec-91a9-aba78f6c932c.jpg)



### **深度学习**

#### **人工智能**

使用模拟、延伸和扩展人的智能的理论、方法、技术和应用的技术科学。

#### **机器学习**

机器学习是专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构，使之不断改善自身的性能。

#### **深度学习**

是机器学习的一种，模型假设、评价函数和优化算法与机器学习一致，其根本差别在于假设的复杂度，其核心算法为**神经网络**。

![深度学习1](https://user-images.githubusercontent.com/4729226/189842281-980a5a95-34c9-49ef-b67e-cfdae1fe6482.png)

### **神经网络基础**

### **基本原理**

模仿神经元细胞来进行的设计，在整体结构上，模仿神经进行分层设计，属于仿生学的算法。

![神经网络](https://user-images.githubusercontent.com/4729226/189842898-032f9b0a-04e0-4d1c-8cf3-4287efc19b7e.png)



- **神经元**： 神经网络中每个节点称为神经元，由两部分组成：
  - 加权和：将所有输入加权求和。
  
  - 非线性变换（激活函数）：加权和的结果经过一个非线性函数变换，让神经元计算具备非线性的能力。
  
- **多层连接**： 大量这样的节点按照不同的层次排布，形成多层的结构连接起来，即称为神经网络。

- **计算图**： 以图形化的方式展现神经网络的计算逻辑又称为计算图。我们也可以将神经网络的计算图以公式的方式表达，如下：Y=f3(f2(f1(w1⋅x1+w2⋅x2+w3⋅x3+b)+…)…)…)

### **正向传播与反向传播**

正向传播本质上是按照输入层到输出层的顺序，求解并保存网络中的中间变量本身。

反向传播本质上是按照输出层到输入层的顺序，求解并保存网络中的中间变量以及其他参数的导数（梯度）。

两者核心都是计算图。

![正向传播](https://user-images.githubusercontent.com/4729226/189844154-f9d46119-a2b7-458d-a551-5b306f93db76.png)

#### **正向传播**

上图中的实际计算过程为：


$$
e=(a+b)*(b+1)
$$


每一步都只能进行原子计算，每个原子计算构成一个圈，继而形成整个计算图。

在计算图中，先进行正向计算c=a+b, d=b+1,再进行e=c∗d，求解得到e的值以后即完成了正向计算的过程。



![反向传播](https://user-images.githubusercontent.com/4729226/189844172-903cc371-4582-4913-8503-f853fef451af.png)

#### **反向传播**

在**前馈计算过程**中，就可以求解得


$$
\frac{\partial c}{\partial a}    \frac{\partial c}{\partial b}
\frac{\partial d}{\partial b}    \frac{\partial d}{\partial 1}
$$


等一系列梯度，这其中所有的梯度信息在正向计算过程中进行保存，并在之后依照**计算图中的链接**，根据**链式法则**反方向求导计算∂e/∂a以及∂e/∂b。



本文参考自[B站《PyTorch深度学习实践》P1](https://www.bilibili.com/video/BV1Y7411d7Ys?p=1)

